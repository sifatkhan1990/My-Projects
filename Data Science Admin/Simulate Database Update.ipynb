{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/psycopg2/__init__.py:144: UserWarning: The psycopg2 wheel package will be renamed from release 2.8; in order to keep installing from binary please use \"pip install psycopg2-binary\" instead. For details see: <http://initd.org/psycopg/docs/install.html#binary-install-from-pypi>.\n",
      "  \"\"\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "# General:\n",
    "import tweepy           # To consume Twitter's API\n",
    "import pandas as pd     # To handle data\n",
    "import numpy as np      # For number computing\n",
    "\n",
    "# For plotting and visualization:\n",
    "from IPython.display import display\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "#test aws\n",
    "import psycopg2\n",
    "from psycopg2 import sql\n",
    "import time\n",
    "## Database details\n",
    "input_db_name = \"osa\"\n",
    "input_db_user = \"janaka\"\n",
    "input_db_password = \"1qazZXC!23\"\n",
    "input_db_port = 5432\n",
    "input_db_host = \"osa.c97j4f5sor7n.eu-west-1.rds.amazonaws.com\"\n",
    "\n",
    "def create_db_connection(db_name=input_db_name, \n",
    "                         db_user=input_db_user,\n",
    "                         db_password=input_db_password, \n",
    "                         db_port=input_db_port,\n",
    "                         db_host=input_db_host):\n",
    "    return psycopg2.connect(host=db_host, \n",
    "                            database=db_name,\n",
    "                            user=db_user, \n",
    "                            password=db_password,\n",
    "                            port=db_port)\n",
    "\n",
    "#machine learning & text \n",
    "from nltk.tokenize import WordPunctTokenizer\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.cross_validation import cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn = create_db_connection()\n",
    "cur = conn.cursor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('template0',), ('rdsadmin',), ('template1',), ('postgres',), ('janaka',), ('osa',)]\n"
     ]
    }
   ],
   "source": [
    "cur.execute(\"\"\"SELECT datname from pg_database\"\"\")\n",
    "rows = cur.fetchall()\n",
    "print(rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create incoming tweets table\n",
    "drop_table_command = \"DROP TABLE incoming_tweets\"\n",
    "cur.execute(drop_table_command)\n",
    "conn.commit()\n",
    "#create_table_command = \"CREATE TABLE incoming_tweets(id serial PRIMARY KEY, sentiment integer NOT NULL, tweet varchar(500))\"\n",
    "create_table_command = \"CREATE TABLE incoming_tweets(id serial PRIMARY KEY, tweet varchar(500))\"\n",
    "cur.execute(create_table_command)\n",
    "conn.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create existing tweets table\n",
    "drop_table_command = \"DROP TABLE existing_tweets\"\n",
    "cur.execute(drop_table_command)\n",
    "conn.commit()\n",
    "create_table_command = \"CREATE TABLE existing_tweets(id serial PRIMARY KEY, tweet varchar(500), sentiment integer NOT NULL)\"\n",
    "cur.execute(create_table_command)\n",
    "conn.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   sentiment                                              tweet  neg_senti\n",
      "0          5  Two places I'd invest all my money if I could:...          0\n",
      "1          5  Awesome! Google driverless cars will help the ...          0\n",
      "2          5  Autonomous vehicles could reduce traffic fatal...          0\n",
      "3          5  Really good presentation from Jan Becker on Bo...          0\n",
      "4          5  Ford just revealed it's Automated Ford Fusion ...          0 (2664, 3)\n"
     ]
    }
   ],
   "source": [
    "#training data\n",
    "df = pd.read_csv('twitter.csv', engine='python')\n",
    "df['neg_senti']= np.where(df['sentiment']<3, 1, 0)\n",
    "print(df.head(), df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dummy Incoming tweet table created\n"
     ]
    }
   ],
   "source": [
    "#generate dummy tweets for testing\n",
    "def gettweets():\n",
    "    \n",
    "    top5=pd.read_csv('twitter.csv', engine='python').head()\n",
    "    \n",
    "    conn = create_db_connection()\n",
    "    cur = conn.cursor()\n",
    "    for i in range(len(top5)):\n",
    "        sql_insert = '''insert into incoming_tweets (tweet)\n",
    "        values ('{0}') \n",
    "        '''.format(top5.tweet[i].replace(\"'\",\"\"))\n",
    "        cur.execute(sql_insert)\n",
    "        conn.commit()\n",
    "    conn.close()\n",
    "    \n",
    "    print('Dummy Incoming tweet table created')\n",
    "\n",
    "gettweets()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#convert tweets from postgres to pandas dataframe\n",
    "def topddf():\n",
    "    \n",
    "    conn = create_db_connection()\n",
    "    cur = conn.cursor()\n",
    "    cur.execute(\"\"\"SELECT * from incoming_tweets limit 2\"\"\")\n",
    "    output = cur.fetchall() \n",
    "    conn.close()\n",
    "    \n",
    "    d = []\n",
    "    for i in range(len(output)):\n",
    "        d.append((output[i][1]))\n",
    "\n",
    "    df = pd.DataFrame(d, columns=['tweet'])\n",
    "    \n",
    "    return df\n",
    "\n",
    "df_test = topddf()\n",
    "len(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['two places invest all money could printing and self driving cars',\n",
       " 'awesome google driverless cars will help the blind travel more often',\n",
       " 'autonomous vehicles could reduce traffic fatalities',\n",
       " 'really good presentation from jan becker bosch automated vehicle research autoauto check out',\n",
       " 'ford just revealed automated ford fusion hybrid vehicle pretty amazing fordtrends ford test',\n",
       " 'yeah just throwing this out there again would totally down beta test autonomous car',\n",
       " 'musk reluctant partner with apple google but android controlled autonomous smart car would awesome',\n",
       " 'finished drive now rush hour for meeting cant wait for autonomous google car',\n",
       " 'the google autonomous car paid visit nvidia pretty cool technology',\n",
       " 'finally very realistic timeline for full autonomous car capability hats off autoforum']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#tweet cleaner 2\n",
    "from nltk.tokenize import WordPunctTokenizer\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "tok = WordPunctTokenizer()\n",
    "pat1 = r'@[A-Za-z0-9]+'\n",
    "pat2 = r'https?://[A-Za-z0-9./]+'\n",
    "combined_pat = r'|'.join((pat1, pat2))   \n",
    "\n",
    "def tweet_cleaner(text):\n",
    "        \n",
    "    soup = BeautifulSoup(text, 'lxml')\n",
    "    souped = soup.get_text()\n",
    "    stripped = re.sub(combined_pat, '', souped)\n",
    "    try:\n",
    "        clean = stripped.decode(\"utf-8-sig\").replace(u\"\\ufffd\", \"?\")\n",
    "    except:\n",
    "        clean = stripped\n",
    "    letters_only = re.sub(\"[^a-zA-Z]\", \" \", clean)\n",
    "    lower_case = letters_only.lower()\n",
    "    # During the letters_only process two lines above, it has created unnecessay white spaces,\n",
    "    # I will tokenize and join together to remove unneccessary white spaces\n",
    "    words = tok.tokenize(lower_case)\n",
    "    \n",
    "    refined_words=[]\n",
    "    for word in words:\n",
    "        if len(word)>2:\n",
    "            refined_words.append(word)\n",
    "            \n",
    "    return (\" \".join(refined_words)).strip()\n",
    "\n",
    "testing = df.tweet[:10]\n",
    "test_result = []\n",
    "for t in testing:\n",
    "    test_result.append(tweet_cleaner(t))\n",
    "test_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CountVectorizer(analyzer='word', binary=True, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=300, min_df=1,\n",
       "        ngram_range=(1, 1), preprocessor=None, stop_words='english',\n",
       "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "        tokenizer=None, vocabulary=None)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['cleaned_tweet'] = np.array([ tweet_cleaner(tweet) for tweet in df.tweet ])\n",
    "titles = df['cleaned_tweet'].fillna('')\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vectorizer = CountVectorizer(max_features = 300, \n",
    "                             ngram_range=(1, 1), \n",
    "                             stop_words='english',\n",
    "                             binary=True)\n",
    "\n",
    "# Use `fit` to learn the vocabulary of the titles\n",
    "vectorizer.fit(titles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trained_model(model,X,y):\n",
    "    \n",
    "    #scoring results\n",
    "    scoresR = cross_val_score(model, X, y, cv=30, scoring='recall')\n",
    "    print('CV Recall {}, Average Recall {}'.format(scoresR, scoresR.mean()))\n",
    "    # F1 = (2 x recall x precision) / (recall + precision)\n",
    "    scoresf1 = cross_val_score(model, X, y, cv=30, scoring='f1')\n",
    "    print('CV F1 {}, Average F1 {}'.format(scoresf1, scoresf1.mean()))\n",
    "    scoresAUC = cross_val_score(model, X, y, cv=30, scoring='roc_auc')\n",
    "    print('CV AUC {}, Average AUC {}'.format(scoresAUC, scoresAUC.mean()))\n",
    "    \n",
    "    #output trained model\n",
    "    trained_model = model.fit(X, y)\n",
    "    \n",
    "    return trained_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "      <th>tweet</th>\n",
       "      <th>neg_senti</th>\n",
       "      <th>cleaned_tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5</td>\n",
       "      <td>Two places I'd invest all my money if I could:...</td>\n",
       "      <td>0</td>\n",
       "      <td>two places invest all money could printing and...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5</td>\n",
       "      <td>Awesome! Google driverless cars will help the ...</td>\n",
       "      <td>0</td>\n",
       "      <td>awesome google driverless cars will help the b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>Autonomous vehicles could reduce traffic fatal...</td>\n",
       "      <td>0</td>\n",
       "      <td>autonomous vehicles could reduce traffic fatal...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5</td>\n",
       "      <td>Really good presentation from Jan Becker on Bo...</td>\n",
       "      <td>0</td>\n",
       "      <td>really good presentation from jan becker bosch...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Ford just revealed it's Automated Ford Fusion ...</td>\n",
       "      <td>0</td>\n",
       "      <td>ford just revealed automated ford fusion hybri...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sentiment                                              tweet  neg_senti  \\\n",
       "0          5  Two places I'd invest all my money if I could:...          0   \n",
       "1          5  Awesome! Google driverless cars will help the ...          0   \n",
       "2          5  Autonomous vehicles could reduce traffic fatal...          0   \n",
       "3          5  Really good presentation from Jan Becker on Bo...          0   \n",
       "4          5  Ford just revealed it's Automated Ford Fusion ...          0   \n",
       "\n",
       "                                       cleaned_tweet  \n",
       "0  two places invest all money could printing and...  \n",
       "1  awesome google driverless cars will help the b...  \n",
       "2  autonomous vehicles could reduce traffic fatal...  \n",
       "3  really good presentation from jan becker bosch...  \n",
       "4  ford just revealed automated ford fusion hybri...  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# select model: logistic regression\n",
    "modellr = LogisticRegression(class_weight= 'balanced') \n",
    "\n",
    "#define model, X and y\n",
    "model = modellr\n",
    "# Use `tranform` to generate the sample X word matrix - one column per feature (word or n-grams)\n",
    "X = vectorizer.transform(titles)\n",
    "y = df.neg_senti\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV Recall [0.73076923 0.76923077 0.73076923 0.65384615 0.65384615 0.76923077\n",
      " 0.80769231 0.61538462 0.26923077 0.46153846 0.5        0.57692308\n",
      " 0.53846154 0.69230769 0.73076923 0.65384615 0.57692308 0.80769231\n",
      " 0.76923077 0.65384615 0.65384615 0.69230769 0.80769231 0.61538462\n",
      " 0.65384615 0.72       0.88       0.72       0.68       0.76      ], Average Recall 0.6714871794871794\n",
      "CV F1 [0.67857143 0.63492063 0.7037037  0.68       0.59649123 0.74074074\n",
      " 0.7        0.47058824 0.26415094 0.375      0.35616438 0.42253521\n",
      " 0.3943662  0.52941176 0.61290323 0.51515152 0.47619048 0.71186441\n",
      " 0.55555556 0.53125    0.52307692 0.61016949 0.6        0.52459016\n",
      " 0.55737705 0.58064516 0.61971831 0.57142857 0.53125    0.52777778], Average F1 0.5531864366323949\n",
      "CV AUC [0.85225885 0.80616606 0.87851038 0.88766789 0.83150183 0.90781441\n",
      " 0.87728938 0.6978022  0.54945055 0.53113553 0.51587302 0.54578755\n",
      " 0.55128205 0.76678877 0.7954823  0.72283272 0.65689866 0.84371184\n",
      " 0.78113553 0.72893773 0.74297924 0.72710623 0.73809524 0.72832723\n",
      " 0.73595849 0.75365079 0.83936508 0.77873016 0.65968254 0.76129032], Average AUC 0.7397837514934289\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lrFeatures</th>\n",
       "      <th>Importance Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>128</th>\n",
       "      <td>jobs</td>\n",
       "      <td>2.351344</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>doesn</td>\n",
       "      <td>1.993579</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>government</td>\n",
       "      <td>1.851629</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132</th>\n",
       "      <td>law</td>\n",
       "      <td>1.563260</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>dont</td>\n",
       "      <td>1.532249</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     lrFeatures  Importance Score\n",
       "128        jobs          2.351344\n",
       "53        doesn          1.993579\n",
       "97   government          1.851629\n",
       "132         law          1.563260\n",
       "55         dont          1.532249"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Output_model = trained_model(model,X,y)\n",
    "\n",
    "#feature importance\n",
    "all_feature_names = vectorizer.get_feature_names()\n",
    "feature_importances = pd.DataFrame({'lrFeatures' : all_feature_names, 'Importance Score': Output_model.coef_[0].tolist()})\n",
    "feature_importances.sort_values('Importance Score', ascending=False).head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "      <th>Predicted_Senti</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Two places Id invest all my money if I could: ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Awesome! Google driverless cars will help the ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               tweet  Predicted_Senti\n",
       "0  Two places Id invest all my money if I could: ...                1\n",
       "1  Awesome! Google driverless cars will help the ...                0"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#predict on incoming tweets\n",
    "test_tweet = vectorizer.transform(df_test.tweet)\n",
    "df_test['Predicted_Senti'] = Output_model.predict(test_tweet).tolist()\n",
    "df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New tweet sentiment pairs added to existing_tweets table\n"
     ]
    }
   ],
   "source": [
    "def updatetweets(df_test):\n",
    "    \n",
    "    conn = create_db_connection()\n",
    "    cur = conn.cursor()\n",
    "\n",
    "    for i in range(len(df_test)):\n",
    "        sql_insert = '''insert into existing_tweets (tweet, sentiment)\n",
    "        values ('{0}', '{1}') \n",
    "        '''.format(df_test.tweet[i].replace(\"'\",\"\"), df_test.Predicted_Senti[i])\n",
    "        cur.execute(sql_insert)\n",
    "        conn.commit()\n",
    "\n",
    "    conn.close()\n",
    "    \n",
    "    print('New tweet sentiment pairs added to existing_tweets table')\n",
    "\n",
    "updatetweets(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All records were deleted from table: incoming_tweets\n",
      "All records were deleted from table: existing_tweets\n"
     ]
    }
   ],
   "source": [
    "def deletetweets(table_name):  \n",
    "    \n",
    "    conn = create_db_connection()\n",
    "    cur = conn.cursor()\n",
    "    cur.execute(\n",
    "        sql.SQL(\"DELETE FROM {}\")\n",
    "            .format(sql.Identifier(table_name)))\n",
    "    conn.commit()\n",
    "    conn.close()\n",
    "    \n",
    "    print('All records were deleted from table: %s'%(table_name))\n",
    "\n",
    "deletetweets('incoming_tweets')\n",
    "deletetweets('existing_tweets')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#simulate demo\n",
    "\n",
    "def demo():\n",
    "    #generate random tweets\n",
    "    gettweets()\n",
    "    #set dummy condition\n",
    "    condition = 1\n",
    "    \n",
    "    while condition != 0:\n",
    "        \n",
    "        #convert osa incoming tweets table to pandas df\n",
    "        df_test = topddf()\n",
    "        \n",
    "        #add predicted sentiment column\n",
    "        test_tweet = vectorizer.transform(df_test.tweet)\n",
    "        df_test['Predicted_Senti'] = Output_model.predict(test_tweet).tolist()\n",
    "        print(df_test.head())\n",
    "        \n",
    "        #update new records into osa existing tweets table\n",
    "        updatetweets(df_test)\n",
    "        \n",
    "        #set condition to stop while loop\n",
    "        deletetweets('incoming_tweets')\n",
    "        \n",
    "        conn = create_db_connection()\n",
    "        cur = conn.cursor()\n",
    "        cur.execute(\"\"\"SELECT * from incoming_tweets limit 1\"\"\")\n",
    "        condition = cur.fetchone() \n",
    "        conn.close()\n",
    "\n",
    "        if condition is None:\n",
    "            condition = 0  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dummy Incoming tweet table created\n",
      "                                               tweet  Predicted_Senti\n",
      "0  Two places Id invest all my money if I could: ...                1\n",
      "1  Awesome! Google driverless cars will help the ...                0\n",
      "New tweet sentiment pairs added to existing_tweets table\n",
      "All records were deleted from table: incoming_tweets\n"
     ]
    }
   ],
   "source": [
    "# test demo\n",
    "demo() "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
