{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# General:\n",
    "import tweepy           # To consume Twitter's API\n",
    "import pandas as pd     # To handle data\n",
    "import numpy as np      # For number computing\n",
    "import dill as pickle   # For saving trained models\n",
    "\n",
    "# For plotting and visualization:\n",
    "from IPython.display import display\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "#machine learning & text \n",
    "from nltk.tokenize import WordPunctTokenizer\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cross_validation import cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import model_selection, svm, metrics, preprocessing, grid_search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model paths\n",
    "path = '/Users/sifatkhan/desktop/indonesia project/models/'\n",
    "file_cv_vect_ngram_model = \"cv_vect_ngram_model.pk\"\n",
    "file_sentiment_model1 = \"sentiment_model1.pk\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   sentiment                                              tweet  neg_senti\n",
      "0          5  Two places I'd invest all my money if I could:...          0\n",
      "1          5  Awesome! Google driverless cars will help the ...          0\n",
      "2          5  Autonomous vehicles could reduce traffic fatal...          0\n",
      "3          5  Really good presentation from Jan Becker on Bo...          0\n",
      "4          5  Ford just revealed it's Automated Ford Fusion ...          0 (2664, 3)\n"
     ]
    }
   ],
   "source": [
    "#training data\n",
    "df = pd.read_csv('twitter.csv', engine='python')\n",
    "df['neg_senti']= np.where(df['sentiment']<3, 1, 0)\n",
    "print(df.head(), df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'name sifat work senior data scientist smart solut'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from collections import defaultdict\n",
    "from nltk.stem import SnowballStemmer\n",
    "def clean(input_text):\n",
    "\n",
    "    stop = set(stopwords.words('english')) \n",
    "    exclude = set(string.punctuation)\n",
    "    lemma = WordNetLemmatizer()\n",
    "    snow = SnowballStemmer('english')\n",
    "\n",
    "    #Filter ASCII characters\n",
    "    non_ascii = ''.join([i if ord(i) < 128 else ' ' for i in input_text])\n",
    "\n",
    "    #remove numbers\n",
    "    number_free = \"\".join([i for i in non_ascii if not i.isdigit()])\n",
    "\n",
    "    # remove punctuation\n",
    "    punc_free = ''.join(ch if ch not in exclude else ' ' for ch in number_free) \n",
    "    \n",
    "    #remove stop words \n",
    "    stop_free = \" \".join([i for i in punc_free.lower().split() if i not in stop])\n",
    "\n",
    "    #Lemmatization\n",
    "    lemmatized = \" \".join(lemma.lemmatize(word) for word in stop_free.split())\n",
    "    \n",
    "    #stemmers\n",
    "    normalized = \" \".join(snow.stem(word) for word in lemmatized.split())\n",
    "    return normalized   \n",
    "\n",
    "clean('my name is sifat and i am working as senior data scientist at smarts solutions.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['two place invest money could print self drive car',\n",
       " 'awesom googl driverless car help blind travel often',\n",
       " 'autonom vehicl could reduc traffic fatal',\n",
       " 'realli good present jan becker bosch autom vehicl research autoauto check',\n",
       " 'ford reveal autom ford fusion hybrid vehicl pretti amaz fordtrend ford test',\n",
       " 'yeah throw would total beta test autonom car',\n",
       " 'musk reluct partner appl googl android control autonom smart car would awesom',\n",
       " 'finish sf la drive rush hour meet cant wait autonom googl car',\n",
       " 'googl autonom car paid visit nvidia hq pretti cool technolog',\n",
       " 'final realist timelin full autonom car capabl hat autoforum']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#tweet cleaner 2\n",
    "from nltk.tokenize import WordPunctTokenizer\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "tok = WordPunctTokenizer()\n",
    "pat1 = r'@[A-Za-z0-9]+'\n",
    "pat2 = r'https?://[A-Za-z0-9./]+'\n",
    "combined_pat = r'|'.join((pat1, pat2))   \n",
    "\n",
    "def tweet_cleaner(text):\n",
    "        \n",
    "    soup = BeautifulSoup(text, 'lxml')\n",
    "    souped = soup.get_text()\n",
    "    stripped = re.sub(combined_pat, '', souped)\n",
    "    try:\n",
    "        clean = stripped.decode(\"utf-8-sig\").replace(u\"\\ufffd\", \"?\")\n",
    "    except:\n",
    "        clean = stripped\n",
    "    letters_only = re.sub(\"[^a-zA-Z]\", \" \", clean)\n",
    "    lower_case = letters_only.lower()\n",
    "    # During the letters_only process two lines above, it has created unnecessay white spaces,\n",
    "    # I will tokenize and join together to remove unneccessary white spaces\n",
    "    words = tok.tokenize(lower_case)\n",
    "    \n",
    "    refined_words=[]\n",
    "    for word in words:\n",
    "        if len(word)>2:\n",
    "            refined_words.append(word)\n",
    "            \n",
    "    return (\" \".join(words)).strip()\n",
    "\n",
    "testing = df.tweet[:10]\n",
    "test_result = []\n",
    "for t in testing:\n",
    "    test_result.append(clean(tweet_cleaner(t)))\n",
    "test_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df['cleaned_tweet'] = np.array([ clean(tweet_cleaner(tweet)) for tweet in df.tweet ])\n",
    "df['cleaned_tweet'] = np.array([tweet_cleaner(tweet) for tweet in df.tweet ])\n",
    "titles = df['cleaned_tweet'].fillna('')\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vectorizer = CountVectorizer(max_features = 300, \n",
    "                             ngram_range=(1, 1), \n",
    "                             stop_words='english',\n",
    "                             binary=True)\n",
    "\n",
    "# Use `fit` to learn the vocabulary of the titles\n",
    "vectorizer.fit(titles)\n",
    "\n",
    "#save trained vectorizer\n",
    "with open(path + file_cv_vect_ngram_model, 'wb') as file_cv_model:\n",
    "    pickle.dump(vectorizer, file_cv_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trained_model(model,X,y):\n",
    "    \n",
    "    #scoring results\n",
    "    scoresAccuracy = cross_val_score(model, X, y, cv=30, scoring='accuracy')\n",
    "    print('CV Accuracy {}, Average Accuracy {}'.format(scoresAccuracy, scoresAccuracy.mean()))\n",
    "    scoresR = cross_val_score(model, X, y, cv=30, scoring='recall')\n",
    "    print('CV Recall {}, Average Recall {}'.format(scoresR, scoresR.mean()))\n",
    "    # F1 = (2 x recall x precision) / (recall + precision)\n",
    "    scoresf1 = cross_val_score(model, X, y, cv=30, scoring='f1')\n",
    "    print('CV F1 {}, Average F1 {}'.format(scoresf1, scoresf1.mean()))\n",
    "    scoresAUC = cross_val_score(model, X, y, cv=30, scoring='roc_auc')\n",
    "    print('CV AUC {}, Average AUC {}'.format(scoresAUC, scoresAUC.mean()))\n",
    "    \n",
    "    #output trained model\n",
    "    trained_model = model.fit(X, y)\n",
    "    \n",
    "    return trained_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "      <th>tweet</th>\n",
       "      <th>neg_senti</th>\n",
       "      <th>cleaned_tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5</td>\n",
       "      <td>Two places I'd invest all my money if I could:...</td>\n",
       "      <td>0</td>\n",
       "      <td>two places i d invest all my money if i could ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5</td>\n",
       "      <td>Awesome! Google driverless cars will help the ...</td>\n",
       "      <td>0</td>\n",
       "      <td>awesome google driverless cars will help the b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>Autonomous vehicles could reduce traffic fatal...</td>\n",
       "      <td>0</td>\n",
       "      <td>autonomous vehicles could reduce traffic fatal...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5</td>\n",
       "      <td>Really good presentation from Jan Becker on Bo...</td>\n",
       "      <td>0</td>\n",
       "      <td>really good presentation from jan becker on bo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Ford just revealed it's Automated Ford Fusion ...</td>\n",
       "      <td>0</td>\n",
       "      <td>ford just revealed it s automated ford fusion ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sentiment                                              tweet  neg_senti  \\\n",
       "0          5  Two places I'd invest all my money if I could:...          0   \n",
       "1          5  Awesome! Google driverless cars will help the ...          0   \n",
       "2          5  Autonomous vehicles could reduce traffic fatal...          0   \n",
       "3          5  Really good presentation from Jan Becker on Bo...          0   \n",
       "4          5  Ford just revealed it's Automated Ford Fusion ...          0   \n",
       "\n",
       "                                       cleaned_tweet  \n",
       "0  two places i d invest all my money if i could ...  \n",
       "1  awesome google driverless cars will help the b...  \n",
       "2  autonomous vehicles could reduce traffic fatal...  \n",
       "3  really good presentation from jan becker on bo...  \n",
       "4  ford just revealed it s automated ford fusion ...  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# select model: logistic regression, linear svc\n",
    "lr = LogisticRegression(class_weight= 'balanced') \n",
    "linearsvc = svm.LinearSVC(random_state=0, class_weight= 'balanced')\n",
    "svc = svm.SVC(C=0.1, gamma=0.1, kernel='rbf', class_weight= 'balanced')\n",
    "\n",
    "#define model, X and y\n",
    "model = lr\n",
    "\n",
    "#unpickle vectorizer\n",
    "# with open(path + file_cv_vect_ngram_model, 'rb') as file_cv_model:\n",
    "#     loaded_vectorizer = pickle.load(file_cv_model)\n",
    "# vectorized_X = loaded_vectorizer.transform(titles)\n",
    "\n",
    "# Use `tranform` to generate the sample X word matrix - one column per feature (word or n-grams)\n",
    "X = vectorizer.transform(titles)\n",
    "y = df.neg_senti\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV Accuracy [0.7752809  0.71910112 0.82022472 0.82022472 0.75280899 0.83146067\n",
      " 0.80898876 0.62921348 0.52808989 0.60674157 0.46067416 0.52808989\n",
      " 0.52808989 0.65168539 0.71910112 0.64044944 0.62921348 0.82022472\n",
      " 0.65168539 0.64044944 0.64044944 0.66292135 0.68539326 0.64044944\n",
      " 0.69662921 0.70454545 0.70454545 0.69318182 0.65909091 0.6091954 ], Average Accuracy 0.6752733162700228\n",
      "CV Recall [0.65384615 0.73076923 0.73076923 0.69230769 0.65384615 0.76923077\n",
      " 0.84615385 0.69230769 0.26923077 0.46153846 0.5        0.65384615\n",
      " 0.57692308 0.69230769 0.73076923 0.65384615 0.57692308 0.84615385\n",
      " 0.76923077 0.61538462 0.69230769 0.53846154 0.80769231 0.57692308\n",
      " 0.65384615 0.72       0.88       0.64       0.68       0.76      ], Average Recall 0.6688205128205128\n",
      "CV F1 [0.62962963 0.6031746  0.7037037  0.69230769 0.60714286 0.72727273\n",
      " 0.72131148 0.52173913 0.25       0.40677966 0.35135135 0.44736842\n",
      " 0.41666667 0.53731343 0.6031746  0.51515152 0.47619048 0.73333333\n",
      " 0.56338028 0.5        0.52941176 0.48275862 0.6        0.48387097\n",
      " 0.55737705 0.58064516 0.62857143 0.54237288 0.53125    0.52777778], Average F1 0.5490342404284194\n",
      "CV AUC [0.85164835 0.80250305 0.87728938 0.88461538 0.82295482 0.90842491\n",
      " 0.88034188 0.72588523 0.54029304 0.56593407 0.52197802 0.55250305\n",
      " 0.55860806 0.78083028 0.77960928 0.72161172 0.65628816 0.83638584\n",
      " 0.79212454 0.71306471 0.73901099 0.65689866 0.74023199 0.69993895\n",
      " 0.73840049 0.75428571 0.83301587 0.77936508 0.65904762 0.75935484], Average AUC 0.7377481323932936\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lrFeatures</th>\n",
       "      <th>Importance Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>125</th>\n",
       "      <td>jobs</td>\n",
       "      <td>2.308946</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>doesn</td>\n",
       "      <td>1.996930</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>government</td>\n",
       "      <td>1.768871</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>129</th>\n",
       "      <td>law</td>\n",
       "      <td>1.569731</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>dont</td>\n",
       "      <td>1.516880</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     lrFeatures  Importance Score\n",
       "125        jobs          2.308946\n",
       "52        doesn          1.996930\n",
       "95   government          1.768871\n",
       "129         law          1.569731\n",
       "54         dont          1.516880"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Output_model = trained_model(model,X,y)\n",
    "\n",
    "# save trained model\n",
    "with open(path + file_sentiment_model1, 'wb') as file_sentiment_model1:\n",
    "    pickle.dump(Output_model, file_sentiment_model1)\n",
    "    \n",
    "#feature importance\n",
    "all_feature_names = vectorizer.get_feature_names()\n",
    "feature_importances = pd.DataFrame({'lrFeatures' : all_feature_names, 'Importance Score': Output_model.coef_[0].tolist()})\n",
    "feature_importances.sort_values('Importance Score', ascending=False).head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "float"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testt= 'hi there'\n",
    "test_tweet = vectorizer.transform([testt])\n",
    "type(float(Output_model.predict(test_tweet)[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_sentiment(text):\n",
    "    text= vectorizer.transform([text])\n",
    "    sentiment = Output_model.predict(text)[0]\n",
    "    return float(sentiment)\n",
    "get_sentiment('government cars')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def svc_param_selection(X, y, nfolds):\n",
    "\n",
    "#     Cs = [0.001, 0.01, 0.1, 1, 10, 100, 110]\n",
    "#     gammas = [0.001, 0.01, 0.1, 1]\n",
    "    Cs =[0.001, 0.01, 0.1]\n",
    "    gammas = [0.001, 0.01, 0.1]\n",
    "\n",
    "    param_grid = {'C': Cs, 'gamma' : gammas}\n",
    "    grid_search = model_selection.GridSearchCV(svm.SVC(kernel='rbf'), param_grid, cv=nfolds)\n",
    "    grid_search.fit(X, y)\n",
    "    grid_search.best_params_\n",
    "    \n",
    "    return grid_search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.001 0.001\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SVC(C=0.001, cache_size=200, class_weight=None, coef0=0.0,\n",
       "  decision_function_shape='ovr', degree=3, gamma=0.001, kernel='rbf',\n",
       "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
       "  tol=0.001, verbose=False)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params = svc_param_selection(X,y, 11) \n",
    "\n",
    "params_C = params.get(\"C\", \"none\")\n",
    "params_gamma = params.get(\"gamma\", \"none\")\n",
    "print(params_C, params_gamma)\n",
    "classifier = svm.SVC(C=params_C, gamma=params_gamma, kernel='rbf')\n",
    "classifier.fit(X,y)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
